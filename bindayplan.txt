Building the front end and the web scraping script separately before integrating them with the server and API. 
step-by-step breakdown:

1. **Web Scraping Script**:
   - Develop the Python web scraping script independently. This script can be tested separately by running it in your terminal, 
   and you can use hard-coded or manually input data initially.
   BeautifulSoup is a web scraper tool for static HTML - it will read the HTML from a page, so at the minute i can read all the page info but it says "loading details" when i search for binsdiv
      <div id="binsdiv">
      <p>
      Loading details....
      </p>
      </div> 
   The content I want is being loaded dynamically by JavaScript, so I need a different tool like Selenium to automate browser actions and interact with the dynamic elements on the page. Need to use Selenium to wait for the "loading details" message to disappear or for the details to become visible, indicating that the dynamic content has been loaded. Once the dynamic content is loaded, you can then use BeautifulSoup to parse the HTML and extract the information you need. 
   Current workaround has been to pull the unique address when inspecting the site with dev tools. Will need to test if this unique url still stands after a change - for the next weeks data.


2. **Front-End Development**:
   - Begin by creating the user interface (HTML and JavaScript) without the server and API.
   - Design the input forms and the display area for the data, but instead of sending data to a server, 
   you can initially use JavaScript to process the data locally or just for user interaction.

3. **Connecting the Parts**:
   - Once you have a functional front end and a working web scraping script, you can build the server and API (using a framework like Flask or Django).
   - Create an API endpoint that the front end can call to send user input to the back end.

4. **Integrate the Web Scraping**:
   - Within the API endpoint, integrate the Python web scraping script to process the user input received from the front end.
   - Retrieve data from web scraping based on the user's input.

5. **Response to the Front End**:
   - Send the scraped data back as a response from the API endpoint.

6. **Update the Front-End JavaScript**:
   - Update the front-end JavaScript code to make an API request to the newly created endpoint and handle the data received from the back end. 
   You'll replace any local processing of data with the data received from the server.

This approach allows you to break down the development process into manageable steps and ensure that each component works properly before integrating them. 
It's a good way to troubleshoot issues, test components individually, and simplify debugging.

Make sure to design your API and front-end code with a consistent data format (e.g., using JSON) to ensure a smooth data exchange between them.


